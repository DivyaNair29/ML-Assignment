{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e9d2cc",
   "metadata": {},
   "source": [
    "1. What is feature engineering, and how does it work? Explain the various aspects of feature\n",
    "engineering in depth.\n",
    "\n",
    "Ans: Feature engineering is the pre-processing step of machine learning, which is used to transform raw data into features that can be used for creating a predictive model using Machine learning or statistical Modelling. Feature engineering in machine learning aims to improve the performance of models. It helps to represent an underlying problem to predictive models in a better way, which as a result, improve the accuracy of the model for unseen data. The predictive model contains predictor variables and an outcome variable, and while the feature engineering process selects the most useful predictor variables for the model.\n",
    "\n",
    "Feature engineering in ML contains mainly four processes: Feature Creation, Transformations, Feature Extraction, and Feature Selection.\n",
    "\n",
    "1) Feature Creation: Feature creation is finding the most useful variables to be used in a predictive model. The process is subjective, and it requires human creativity and intervention. The new features are created by mixing existing features using addition, subtraction, and ration, and these new features have great flexibility.\n",
    "    \n",
    "2) Transformations: The transformation step of feature engineering involves adjusting the predictor variable to improve the accuracy and performance of the model. For example, it ensures that the model is flexible to take input of the variety of data; it ensures that all the variables are on the same scale, making the model easier to understand. It improves the model's accuracy and ensures that all the features are within the acceptable range to avoid any computational error.\n",
    "    \n",
    "3) Feature Extraction: Feature extraction is an automated feature engineering process that generates new variables by extracting them from the raw data. The main aim of this step is to reduce the volume of data so that it can be easily used and managed for data modelling. Feature extraction methods include cluster analysis, text analytics, edge detection algorithms, and principal components analysis (PCA).\n",
    "    \n",
    "4) Feature Selection: While developing the machine learning model, only a few variables in the dataset are useful for building the model, and the rest features are either redundant or irrelevant. If we input the dataset with all these redundant and irrelevant features, it may negatively impact and reduce the overall performance and accuracy of the model. Hence it is very important to identify and select the most appropriate features from the data and remove the irrelevant or less important features, which is done with the help of feature selection in machine learning. \"Feature selection is a way of selecting the subset of the most relevant features from the original features set by removing the redundant, irrelevant, or noisy features.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60c8b87",
   "metadata": {},
   "source": [
    "2. What is feature selection, and how does it work? What is the aim of it? What are the various\n",
    "methods of function selection?\n",
    "\n",
    "Ans: Same as answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb59a238",
   "metadata": {},
   "source": [
    "3. Describe the function selection filter and wrapper approaches. State the pros and cons of each\n",
    "approach?\n",
    "\n",
    "Ans: \n",
    "Filter methods: In this method, features are filtered based on general characteristics (some metric such as correlation) of the dataset such correlation with the dependent variable. Filter method is performed without any predictive model. It is faster and usually the better approach when the number of features are huge. Avoids overfitting but sometimes may fail to select best features.\n",
    "\n",
    "Wrapper method: In wrapper method, the feature selection algorithm exits as a wrapper around the predictive model algorithm and uses the same model to select best features (more on this from this excellent research paper). Though computationally expensive and prone to overfitting, gives better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2dabea",
   "metadata": {},
   "source": [
    "4.i. Describe the overall feature selection process.\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most\n",
    "widely used function extraction algorithms?\n",
    "\n",
    "Ans: i) Feature Selection: While developing the machine learning model, only a few variables in the dataset are useful for building the model, and the rest features are either redundant or irrelevant. If we input the dataset with all these redundant and irrelevant features, it may negatively impact and reduce the overall performance and accuracy of the model. Hence it is very important to identify and select the most appropriate features from the data and remove the irrelevant or less important features, which is done with the help of feature selection in machine learning. \"Feature selection is a way of selecting the subset of the most relevant features from the original features set by removing the redundant, irrelevant, or noisy features.\"\n",
    "\n",
    "ii) Feature extraction is a subset of feature engineering. Data scientists turn to feature extraction when the data in its raw form is unusable. Feature extraction transforms raw data into numerical features compatible with machine learning algorithms. One common application is raw data in the form of image filesâ€”by extracting the shape of an object or the redness value in images, data scientists can create new features suitable for machine learning applications. Autoencoders, wavelet scattering, and deep neural networks are commonly used to extract features and reduce dimensionality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b17bf",
   "metadata": {},
   "source": [
    "6. What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
    "two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in\n",
    "cosine.\n",
    "\n",
    "Ans: Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure document similarity in text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "540058d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (2, 3, 2, 0, 2, 3, 3, 0, 1)\n",
    "y = (2, 1, 0, 0, 3, 2, 1, 3, 1)\n",
    "\n",
    "x_y = (2*2)+(3*1)+(2*0)+(0*0)+(2*3)+(3*2)+(3*1)+(0*3)+(1*1)\n",
    "x_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedeabc4",
   "metadata": {},
   "source": [
    "||x|| = 38\n",
    "\n",
    "||y|| = 27\n",
    "\n",
    "||x|| * ||y|| = 1026\n",
    "\n",
    "Cos(x, y) = x_y / ||x|| * ||y||\n",
    "          = 23 /1026 = 0.022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec871c",
   "metadata": {},
   "source": [
    "7.What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
    "calculate the Hamming gap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d411a1",
   "metadata": {},
   "source": [
    "Ans:Hamming distance is a metric for comparing two binary data strings. While comparing two binary strings of equal length, Hamming distance is the number of bit positions in which the two bits are different.\n",
    "\n",
    "d(10001011,11001111) = 01000100 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb07aa0",
   "metadata": {},
   "source": [
    "8. State what is meant by &quot;high-dimensional data set&quot;? Could you offer a few real-life examples?\n",
    "What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
    "What can be done about it?\n",
    "\n",
    "Ans: High-dimensional data are defined as data in which the number of features (variables observed), p, are close to or larger than the number of observations (or data points), n. The opposite is low-dimensional data in which the number of observations, n, far outnumbers the number of features, p. Data on health status of patients can be high-dimensional (100+ measured/recorded parameters from blood analysis, immune system status, genetic background, nutrition, alcohol- tobacco- drug-consuption, operations, treatments, diagnosed diseases, ...)\n",
    "The difficulty in using machine learning techniques on a data set with many dimensions can be the complexity and high computational task. Feature Engineering can be one of the solution to reduce the difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8d419b",
   "metadata": {},
   "source": [
    "9. Make a few quick notes on:\n",
    "\n",
    "1. PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "2. Use of vectors\n",
    "\n",
    "3. Embedded technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8bdaa6",
   "metadata": {},
   "source": [
    "i) PCA - Principal component analysis (PCA) is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data.\n",
    "\n",
    "ii) Use of vectors - Vectors are used throughout the field of machine learning in the description of algorithms and processes such as the target variable (y) when training an algorithm. Vectors are commonly used in machine learning as they lend a convenient way to organize data.\n",
    "\n",
    "iii) Embedded technique - It's implemented by algorithms that have their own feature selection methods in them. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification/regression at the same time. Approaches of Embedded Methods:\n",
    "Regularization Approach:The Regularization approach that includes Lasso(L1 regularization) and Ridge(L2 regularization) and Elastic Nets(L1 and L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8482f7",
   "metadata": {},
   "source": [
    "10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "3. SMC vs. Jaccard coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390efc92",
   "metadata": {},
   "source": [
    "i) Sequential backward exclusion vs. sequential forward selection - Forward selection starts with a (usually empty) set of variables and adds variables to it, until some stop- ping criterion is met. Similarly, backward selection starts with a (usually complete) set of variables and then excludes variables from that set, again, until some stopping criterion is met.\n",
    "\n",
    "ii) filter vs. wrapper - Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "\n",
    "iii) SMC vs. Jaccard coefficient - the SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index only counts mutual presence as matches and compares it to the number of attributes that have been chosen by at least one of the two sets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
