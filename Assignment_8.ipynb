{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e109fb6c",
   "metadata": {},
   "source": [
    "1. What exactly is a feature? Give an example to illustrate your point.\n",
    "\n",
    "Ans: In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition, classification and regression. Each feature, or column, represents a measurable piece of data that can be used for analysis: Name, Age, Sex, Fare, and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a3c15",
   "metadata": {},
   "source": [
    "2. What are the various circumstances in which feature construction is required?\n",
    "\n",
    "Ans: Feature (variable) importance indicates how much each feature contributes to the model prediction. Basically, it determines the degree of usefulness of a specific variable for a current model and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa432d8c",
   "metadata": {},
   "source": [
    "3. Describe how nominal variables are encoded.\n",
    "\n",
    "Ans: When we have a feature where variables are just names and there is no order or rank to this variable's feature. Nominal features can be encoded using One Hot Encoding method. It maps each category with binary numbers (0 or 1). This type of encoding is used when the data is nominal. Newly created binary features can be considered dummy variables. After one hot encoding, the number of dummy variables depends on the number of categories presented in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53e878",
   "metadata": {},
   "source": [
    "4. Describe how numeric features are converted to categorical features.\n",
    "\n",
    "Ans: Discretization: It is the process of transforming continuous variables into categorical variables by creating a set of intervals, which are contiguous, that span over the range of the variable's values. It is also known as “Binning”, where the bin is an analogous name for an interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998b1ec",
   "metadata": {},
   "source": [
    "5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this\n",
    "approach?\n",
    "\n",
    "Ans: In wrapper methods, the feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a820c31",
   "metadata": {},
   "source": [
    "6. When is a feature considered irrelevant? What can be said to quantify it?\n",
    "\n",
    "Ans: The feature which did not require for predicting accuracy is known as an irrelevant feature. The relevancy of the feature is measured based on the characteristics of the data not by its value. \n",
    "Measures of Feature Relevance: In the case of supervised learning, mutual information is considered as a good measure of information contribution of a feature to decide the value of the class label. That is why it is a good indicator of the relevance of a feature with respect to the class variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb0f47",
   "metadata": {},
   "source": [
    "7. When is a function considered redundant? What criteria are used to identify features that could\n",
    "be redundant?\n",
    "\n",
    "Ans: A feature may contribute to information that is similar to the information contributed by one or more features. For example, in the Student Data-set, both the features Age & Height contribute similar information. This is because, with an increase in age, weight is expected to increase. Similarly, with the increase in Height also weight is expected to increase. So, in context to that problem, Age and Height contribute similar information. In other words, irrespective of whether the feature Height is present or not, the learning model will give the same results. In this kind of situation where one feature is similar to another feature, the feature is said to be potentially redundant in the context of a machine learning problem. There are multiple measures of similarity of information contribution, the main ones are:\n",
    "\n",
    "    a) Correlation-based Measures\n",
    "    b) Distance-based Measures\n",
    "    c) Other coefficient-based Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3965b5f5",
   "metadata": {},
   "source": [
    "8. What are the various distance measurements used to determine feature similarity?\n",
    "\n",
    "Ans: Different types of similarity measures exist for various types of objects, depending on the objects being compared. For each type of object there are various similarity measurement formulas.There are many various options available when it comes to finding similarity between two data points, some of which are a combination of other similarity methods. Some of the methods for similarity measures between two data points include Euclidean distance, Manhattan distance, Minkowski distance, and Chebyshev distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0cab98",
   "metadata": {},
   "source": [
    "9. State difference between Euclidean and Manhattan distances?\n",
    "\n",
    "Ans: Euclidean Distance represents the shortest distance between two points. The Euclidean is often the “default” distance used in e.g., K-nearest neighbors (classification) or K-means (clustering) to find the “k closest points” of a particular sample point. The “closeness” is defined by the difference (“distance”) along the scale of each variable, which is converted to a similarity measure. This distance is defined as the Euclidian distance.\n",
    "\n",
    "Manhattan Distance is the sum of absolute differences between points across all the dimensions. Manhattan distance is a metric in which the distance between two points is the sum of the absolute differences of their Cartesian coordinates. In a simple way of saying it is the total sum of the difference between the x-coordinates and y-coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eef4f3",
   "metadata": {},
   "source": [
    "10. Distinguish between feature transformation and feature selection.\n",
    "\n",
    "Ans: feature transformation: transformation of data to improve the accuracy of the algorithm; feature selection: removing unnecessary features.\n",
    "\n",
    "There are 3 types of Feature transformation techniques:\n",
    "\n",
    "    1)Function Transformers\n",
    "    2)Power Transformers\n",
    "    3)Quantile Transformers\n",
    "    \n",
    "Types of feature selection methods are:\n",
    "\n",
    "    1) Filter Methods\n",
    "    2) Wrapper Methods\n",
    "    3) Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7430fc1",
   "metadata": {},
   "source": [
    "11. Make brief notes on any two of the following:\n",
    "\n",
    "1.SVD (Standard Variable Diameter Diameter)\n",
    "\n",
    "2. Collection of features using a hybrid approach\n",
    "\n",
    "3. The width of the silhouette\n",
    "\n",
    "4. Receiver operating characteristic curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f896436f",
   "metadata": {},
   "source": [
    "Ans: 1.SVD - The Singular Value Decomposition (SVD) of a matrix is a factorization of that matrix into three matrices. It has some interesting algebraic properties and conveys important geometrical and theoretical insights about linear transformations. It also has some important applications in data science. \n",
    "\n",
    "2. Collection of features using a hybrid approach - A hybrid feature selection method is proposed for classification in small sample size data sets. The filter step is based on instance learning taking advantage of the small sample size of data. A few candidate feature subsets are generated since their number corresponds to the number of instances.\n",
    "\n",
    "3. The width of the silhouette - The Silhouette Coefficient is calculated using the mean intra-cluster distance ( a ) and the mean nearest-cluster distance ( b ) for each sample. The Silhouette Coefficient for a sample is (b - a) / max(a, b) . To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of.\n",
    "\n",
    "4. Receiver operating characteristic curve - An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: True Positive Rate. False Positive Rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
