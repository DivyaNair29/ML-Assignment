{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c26518ee",
   "metadata": {},
   "source": [
    "1. What is prior probability? Give an example.\n",
    "\n",
    "Ans: Prior probability, in Bayesian statistics, is the probability of an event before new data is collected. This is the best rational assessment of the probability of an outcome based on the current knowledge before an experiment is performed. For example, three acres of land have the labels A, B, and C. One acre has reserves of oil below its surface, while the other two do not. The prior probability of oil being found on acre C is one third, or 0.333."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccbc670",
   "metadata": {},
   "source": [
    "2. What is posterior probability? Give an example.\n",
    "\n",
    "Ans: A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information. The posterior probability is calculated by updating the prior probability using Bayes' theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d22a37",
   "metadata": {},
   "source": [
    "3. What is likelihood probability? Give an example.\n",
    "\n",
    "Ans: The term Likelihood refers to the process of determining the best data distribution given a specific situation in the data. When calculating the probability of a given outcome, you assume the model's parameters are reliable. Suppose we have a coin that is assumed to be fair. If we flip the coin one time, the probability that it will land on heads is 0.5. Now suppose we flip the coin 100 times and it only lands on heads 17 times. We would say that the likelihood that the coin is fair is quite low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b37d6",
   "metadata": {},
   "source": [
    "4. What is Naïve Bayes classifier? Why is it named so?\n",
    "\n",
    "Ans: Naive Bayes classifiers are a collection of classification algorithms based on Bayes’ Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other.\n",
    "Naïve Bayes classification is called Naïve because it assumes class conditional independence. The effect of an attribute value on a given class is independent of the values of the other attributes. This assumption is made to reduce computational costs and hence is considered Naïve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46dfdea",
   "metadata": {},
   "source": [
    "5. What is optimal Bayes classifier?\n",
    "\n",
    "Ans: The Bayes optimal classifier is a probabilistic model that makes the most probable prediction for a new example, given the training dataset. This model is also referred to as the Bayes optimal learner, the Bayes classifier, Bayes optimal decision boundary, or the Bayes optimal discriminant function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011546a9",
   "metadata": {},
   "source": [
    "6. Write any two features of Bayesian learning methods.\n",
    "\n",
    "Ans: In Bayesian learning, prior knowledge is provided by asserting (1) a prior probability for each candidate hypothesis, and (2) a probability distribution over observed data for each possible hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd69a7",
   "metadata": {},
   "source": [
    "7. Define the concept of consistent learners.\n",
    "\n",
    "Ans: A learner L using a hypothesis H and training data D is said to be a consistent learner if it always outputs a hypothesis with zero error on D whenever H contains such a hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948ec83",
   "metadata": {},
   "source": [
    "8. Write any two strengths of Bayes classifier.\n",
    "\n",
    "Ans: 1) It is simple and easy to implement\n",
    "\n",
    "    2)It doesn’t require as much training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97bcc2d",
   "metadata": {},
   "source": [
    "9. Write any two weaknesses of Bayes classifier.\n",
    "\n",
    "Ans: 1) The Naive Bayes Algorithm has trouble with the 'zero-frequency problem'.\n",
    "\n",
    "    2) It will assume that all the attributes are independent, which rarely happens in real life."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e301427",
   "metadata": {},
   "source": [
    "10. Explain how Naïve Bayes classifier is used for\n",
    "\n",
    "1. Text classification\n",
    "\n",
    "2. Spam filtering\n",
    "\n",
    "3. Market sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e5105",
   "metadata": {},
   "source": [
    "1) In order to leverage the power of bayesian text classification, texts have to be transformed into vectors before classification. Since a Naive Bayes text classifier is based on the Bayes’s Theorem, which helps us compute the conditional probabilities of occurrence of two events based on the probabilities of occurrence of each individual event, encoding those probabilities is extremely useful.\n",
    "\n",
    "Using Naive Bayes text classifiers might be a really good idea, especially if there’s not much training data available and computational resources are scarce. Usually, results are pretty competitive in terms of performance if features are well engineered. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a2528",
   "metadata": {},
   "source": [
    "2) Modern spam filtering software continuously struggles to categorise the emails correctly. Unwanted spam & promotional communication is the toughest of them all. Spam communication algorithms must be iterated continuously since there is an ongoing battle between spam filtering software and anonymous spam & promotional mail senders. Naive Bayes Algorithm in data analytics forms the base for text filtering in Gmail, Yahoo Mail, Hotmail & all other platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec647d",
   "metadata": {},
   "source": [
    "3) Sentiment analysis is a popular application of the Naive Bayes algorithm, which is used to determine the sentiment of a text, whether it is positive, negative, or neutral. Bayes theorem provides a way of calculating the posterior probability, P(c|x), from P(c), P(x), and P(x|c). Naive Bayes classifier assume that the effect of the value of a predictor x on a given class c is independent of the values of other predictors. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
